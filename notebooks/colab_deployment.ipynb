{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7111a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üõ†Ô∏è SCRIPT 1: SETUP & INSTALLATION\n",
    "# ==========================================\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 1. Clone the Repository\n",
    "if not os.path.exists('Personal-Voice-Clone'):\n",
    "    print(\"üìÇ Cloning Repository...\")\n",
    "    !git clone https://github.com/micccon/Personal-AI-Clone.git\n",
    "    %cd Personal-Voice-Clone\n",
    "else:\n",
    "    %cd Personal-Voice-Clone\n",
    "    print(\"üìÇ Repo already cloned.\")\n",
    "\n",
    "# 2. Install System Audio Tools\n",
    "print(\"‚öôÔ∏è Installing System Libraries...\")\n",
    "!apt-get update -y > /dev/null 2>&1\n",
    "!apt-get install -y ffmpeg > /dev/null 2>&1\n",
    "\n",
    "# 3. Install Numpy Constraint\n",
    "print(\"üõ°Ô∏è Installing Numpy Constraint...\")\n",
    "!pip install \"numpy<2.0.0\" --force-reinstall\n",
    "\n",
    "# 4. Install Llama-3 (GPU)\n",
    "print(\"üöÄ Installing Llama-3 (GPU)...\")\n",
    "!pip install llama-cpp-python \\\n",
    "  --prefer-binary \\\n",
    "  --extra-index-url=https://abetlen.github.io/llama-cpp-python/whl/cu121 \\\n",
    "  --no-cache-dir\n",
    "\n",
    "# 5. Install App Dependencies (Voice is installed but optional in usage)\n",
    "print(\"‚öôÔ∏è Installing App Dependencies...\")\n",
    "!pip install -q \\\n",
    "    elevenlabs \\\n",
    "    streamlit \\\n",
    "    python-dotenv \\\n",
    "    \"transformers==4.46.1\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ INSTALL COMPLETE.\")\n",
    "print(\"üëâ Please click 'Restart Session' (top menu) if prompted.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d0d6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üîë SCRIPT 2: CONFIGURATION\n",
    "# ==========================================\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "print(\"üìù SETTING UP...\\n\")\n",
    "\n",
    "# 1. Ngrok (REQUIRED for TCP Tunnel)\n",
    "print(\"1Ô∏è‚É£ Enter Ngrok Auth Token (Verified account required for TCP):\")\n",
    "print(\"   (Get it from https://dashboard.ngrok.com/get-started/your-authtoken)\")\n",
    "ngrok_token = getpass()\n",
    "\n",
    "# 2. ElevenLabs (Optional)\n",
    "print(\"\\n2Ô∏è‚É£ Enter ElevenLabs API Key (Optional - press Enter to skip):\")\n",
    "eleven_key = getpass()\n",
    "print(\"\\n3Ô∏è‚É£ Enter Voice ID (Optional - press Enter to skip):\")\n",
    "voice_id = input()\n",
    "\n",
    "# 3. Model Selection (Custom vs Generic)\n",
    "print(\"\\n4Ô∏è‚É£ Model Configuration\")\n",
    "print(\"   Press ENTER to use the generic Llama-3 demo model.\")\n",
    "print(\"   OR paste a direct download link to your own .gguf model (HuggingFace/Dropbox).\")\n",
    "custom_model_url = input(\"   Model URL: \").strip()\n",
    "\n",
    "# --- SAVE CONFIGURATION ---\n",
    "if ngrok_token.strip(): \n",
    "    os.environ[\"NGROK_AUTH_TOKEN\"] = ngrok_token\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    if eleven_key.strip(): f.write(f\"ELEVENLABS_API_KEY={eleven_key}\\n\")\n",
    "    if voice_id.strip(): f.write(f\"VOICE_ID={voice_id}\\n\")\n",
    "\n",
    "# --- DOWNLOAD MODEL ---\n",
    "if not os.path.exists(\"models\"): os.makedirs(\"models\")\n",
    "model_path = \"models/inference_model.gguf\"\n",
    "\n",
    "if custom_model_url:\n",
    "    print(f\"\\n‚¨áÔ∏è Downloading custom model from provided URL...\")\n",
    "    !wget -q \"{custom_model_url}\" -O {model_path}\n",
    "else:\n",
    "    # Default to a high-quality quantized Llama-3-8B-Instruct\n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"\\n‚¨áÔ∏è Downloading generic Llama-3 model for demo purposes...\")\n",
    "        !wget -q https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf -O {model_path}\n",
    "\n",
    "with open(\".env\", \"a\") as f: f.write(f\"MODEL_PATH={model_path}\\n\")\n",
    "print(\"‚úÖ Configuration Saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee734be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# üöÄ SCRIPT 3: LAUNCH APP\n",
    "# ==========================================\n",
    "from pyngrok import ngrok, conf\n",
    "import os\n",
    "\n",
    "# 1. Authenticate\n",
    "if \"NGROK_AUTH_TOKEN\" in os.environ:\n",
    "    ngrok.set_auth_token(os.environ[\"NGROK_AUTH_TOKEN\"])\n",
    "else:\n",
    "    print(\"‚ùå Error: Ngrok token missing. Run Cell 2.\")\n",
    "\n",
    "# 2. Kill old tunnels\n",
    "ngrok.kill()\n",
    "\n",
    "# 3. Open TCP Tunnel\n",
    "# We use TCP because standard HTTP tunnels often fail with Streamlit WebSockets on Colab.\n",
    "try:\n",
    "    tunnel = ngrok.connect(8501, \"tcp\")\n",
    "    \n",
    "    # Convert \"tcp://0.tcp.ngrok.io:12345\" -> \"http://0.tcp.ngrok.io:12345\"\n",
    "    public_url = tunnel.public_url.replace(\"tcp://\", \"http://\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üöÄ YOUR APP IS LIVE! CLICK HERE: {public_url}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # 4. Run Streamlit\n",
    "    !streamlit run app.py \\\n",
    "      --server.port 8501 \\\n",
    "      --server.address 0.0.0.0 \\\n",
    "      --server.headless true\n",
    "      \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Tunnel Error: {e}\")\n",
    "    print(\"‚ö†Ô∏è Note: TCP tunnels require a verified Ngrok account. If you haven't added a payment method to Ngrok, this will fail.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
